import streamlit as st
import arxiv
import feedparser
from openai import OpenAI
import datetime

# --- 1. ç½‘é¡µè®¾ç½® ---
st.set_page_config(page_title="å…·èº«æ™ºèƒ½ & è‡ªåŠ¨é©¾é©¶æ—¥æŠ¥", page_icon="ğŸ¤–", layout="wide")
st.title("ğŸ¤– å…·èº«æ™ºèƒ½ & è‡ªåŠ¨é©¾é©¶æƒ…æŠ¥ç«™")

# --- 2. ä¾§è¾¹æ ï¼šè®¾ç½®ä¸æ§åˆ¶ ---
with st.sidebar:
    st.header("âš™ï¸ è®¾ç½®é¢æ¿")
    
    # è·å– API Key (ä¼˜å…ˆä»ç³»ç»Ÿè¯»å–ï¼Œè¯»ä¸åˆ°å°±è®©ç”¨æˆ·å¡«)
    api_key = st.secrets.get("DEEPSEEK_API_KEY", None)
    if not api_key:
        api_key = st.text_input("è¯·è¾“å…¥ DeepSeek/OpenAI API Key:", type="password")
        if not api_key:
            st.warning("âš ï¸ æœªæ£€æµ‹åˆ° API Keyï¼ŒAI æ€»ç»“åŠŸèƒ½å°†ä¸å¯ç”¨")
    
    base_url = st.text_input("API åœ°å€", value="https://api.deepseek.com")
    model_name = st.text_input("æ¨¡å‹åç§°", value="deepseek-chat")
    
    st.divider()
    st.subheader("å…³é”®è¯è®¾ç½®")
    # é»˜è®¤æœç´¢å…³é”®è¯
    default_keywords = "Embodied AI\nAutonomous Driving\nHumanoid Robot\nEnd-to-end Driving"
    keywords_input = st.text_area("è¾“å…¥å…³é”®è¯ (æ¯è¡Œä¸€ä¸ª)", value=default_keywords, height=150)
    keywords = [k.strip() for k in keywords_input.split('\n') if k.strip()]

# --- 3. åŠŸèƒ½å‡½æ•°ï¼šAI æ€»ç»“ ---
def get_ai_summary(text):
    if not api_key:
        return None
    
    client = OpenAI(api_key=api_key, base_url=base_url)
    try:
        prompt = f"è¯·ç”¨ä¸­æ–‡ä¸€å¥è¯æ€»ç»“è¿™ç¯‡å…³äº{keywords[0]}çš„æ–‡ç« æ ¸å¿ƒï¼Œå¹¶åˆ—å‡º3ä¸ªå…³é”®ç‚¹ã€‚\n\nåŸæ–‡ï¼š{text[:1000]}"
        response = client.chat.completions.create(
            model=model_name,
            messages=[{"role": "user", "content": prompt}]
        )
        return response.choices[0].message.content
    except Exception as e:
        return f"AI æ€»ç»“å‡ºé”™: {e}"

# --- 4. åŠŸèƒ½å‡½æ•°ï¼šè·å–æ•°æ® ---
def get_arxiv_papers():
    search_query = " OR ".join([f'ti:"{k}"' for k in keywords])
    # æœç´¢æœ€è¿‘æäº¤çš„
    search = arxiv.Search(query=search_query, max_results=5, sort_by=arxiv.SortCriterion.SubmittedDate)
    
    results = []
    client = arxiv.Client()
    for r in client.results(search):
        results.append({
            "title": r.title,
            "link": r.pdf_url,
            "summary": r.summary,
            "date": r.published.date()
        })
    return results

# --- 5. é¡µé¢å±•ç¤ºé€»è¾‘ ---
tab1, tab2 = st.tabs(["ğŸ“„ æœ€æ–°è®ºæ–‡ (Arxiv)", "ğŸŒ äº§ä¸šæ–°é—» (RSS)"])

with tab1:
    if st.button("ğŸ” æ‰«ææœ€æ–°è®ºæ–‡"):
        with st.spinner("æ­£åœ¨è¿æ¥ Arxiv æ•°æ®åº“..."):
            papers = get_arxiv_papers()
            st.success(f"æ‰¾åˆ° {len(papers)} ç¯‡æœ€æ–°è®ºæ–‡")
            
            for p in papers:
                with st.expander(f"[{p['date']}] {p['title']}"):
                    st.write(f"**åŸæ–‡é“¾æ¥**: {p['link']}")
                    if st.button("âœ¨ AI è§£è¯»", key=p['link']):
                        summary = get_ai_summary(p['summary'])
                        if summary:
                            st.info(summary)
                    else:
                        st.caption("ç‚¹å‡»ä¸Šæ–¹æŒ‰é’®æŸ¥çœ‹ AI ä¸­æ–‡æ€»ç»“")
                        st.text(p['summary'])

with tab2:
    st.info("ğŸ’¡ æç¤ºï¼šè¿™é‡Œæ¼”ç¤ºä» TechCrunch è·å– AI æ–°é—»")
    # è¿™é‡Œç”¨ä¸€ä¸ªç¨³å®šçš„å›½å¤–ç§‘æŠ€æºåšæ¼”ç¤º
    rss_url = "https://techcrunch.com/category/artificial-intelligence/feed/"
    
    if st.button("ğŸ” æ‰«ææœ€æ–°æ–°é—»"):
        feed = feedparser.parse(rss_url)
        for entry in feed.entries[:5]:
            st.markdown(f"**[{entry.published[:16]}] {entry.title}**")
            st.markdown(f"[é˜…è¯»åŸæ–‡]({entry.link})")
            if st.button("âœ¨ AI æ‘˜è¦", key=entry.link):
                # ç»„åˆæ ‡é¢˜å’Œæ‘˜è¦å‘ç»™ AI
                content = entry.title + "\n" + (entry.get('summary', '') or entry.get('description', ''))
                summary = get_ai_summary(content)
                if summary:
                    st.success(summary)
            st.divider()
